import cv2
import numpy as np
from PIL import Image, ImageFilter
import matplotlib.pyplot as plt

# 画像読み込み
image_path = 'sample_image.jpg'  # 適切な画像ファイルを指定
image = Image.open(image_path)

# 1. 画像のリサイズ
resized_image = image.resize((300, 300))

# 2. 画像のフィルタリング（ぼかしとシャープ化）
blurred_image = image.filter(ImageFilter.GaussianBlur(5))
sharpened_image = image.filter(ImageFilter.SHARPEN)

# 3. 画像の回転、反転
rotated_image = image.rotate(45)  # 45度回転
flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)  # 左右反転

# 4. 画像の合成（ぼかしとシャープ化を合成）
composite_image = Image.blend(blurred_image, sharpened_image, alpha=0.5)

# 5. 画像のエッジ検出（Sobelエッジ検出）
image_cv = np.array(image)
gray_image = cv2.cvtColor(image_cv, cv2.COLOR_RGB2GRAY)
sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)
sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)
edges_sobel = cv2.magnitude(sobel_x, sobel_y)

# 6. 画像の変換（グレースケール、二値化）
gray_image = cv2.cvtColor(image_cv, cv2.COLOR_RGB2GRAY)
_, binary_image = cv2.threshold(gray_image, 127, 255, cv2.THRESH_BINARY)

# 7. 画像の輪郭検出
contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
contour_image = cv2.drawContours(np.copy(image_cv), contours, -1, (0, 255, 0), 2)

# 8. 動画のフレーム処理（OpenCVで動画読み込み）
video_path = 'sample_video.mp4'  # 適切な動画ファイルを指定
cap = cv2.VideoCapture(video_path)
ret, frame = cap.read()  # 1フレーム目を読み込む
if ret:
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    plt.imshow(frame_gray, cmap='gray')
    plt.title("First Frame Grayscale")
    plt.show()

cap.release()

# 9. 顔検出（Haarカスケード）
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

# 顔を矩形で描画
image_with_faces = image_cv.copy()
for (x, y, w, h) in faces:
    cv2.rectangle(image_with_faces, (x, y), (x+w, y+h), (0, 255, 0), 2)

# 10. 画像分類（CNN: ここではCNNモデルの準備例）
# 通常、画像分類には学習済みのCNNモデル（例えばVGG16やResNet）を使用します。
# 下記はモデルの準備部分のみ示します。

# import tensorflow as tf
# from tensorflow.keras.applications import VGG16
# model = VGG16(weights='imagenet', include_top=True)
# img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))
# img_array = tf.keras.preprocessing.image.img_to_array(img)
# img_array = np.expand_dims(img_array, axis=0)
# img_array = tf.keras.applications.vgg16.preprocess_input(img_array)
# predictions = model.predict(img_array)
# decoded_predictions = tf.keras.applications.vgg16.decode_predictions(predictions)
# print(decoded_predictions)

# 11. 物体検出（YOLO：適切なYOLOモデルを用いる）
# YOLOを用いた物体検出の基本的な処理は次の通りです：
# まずYOLOのモデルとクラスファイルを読み込んで、物体検出を行います。

# yolo_net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
# layer_names = yolo_net.getLayerNames()
# output_layers = [layer_names[i[0] - 1] for i in yolo_net.getLayers()]
# blob = cv2.dnn.blobFromImage(image_cv, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
# yolo_net.setInput(blob)
# detections = yolo_net.forward(output_layers)

# 画像の表示
fig, axes = plt.subplots(3, 4, figsize=(15, 10))
axes[0, 0].imshow(resized_image)
axes[0, 0].set_title('Resized Image')
axes[0, 1].imshow(blurred_image)
axes[0, 1].set_title('Blurred Image')
axes[0, 2].imshow(sharpened_image)
axes[0, 2].set_title('Sharpened Image')
axes[0, 3].imshow(rotated_image)
axes[0, 3].set_title('Rotated Image')
axes[1, 0].imshow(flipped_image)
axes[1, 0].set_title('Flipped Image')
axes[1, 1].imshow(composite_image)
axes[1, 1].set_title('Composite Image')
axes[1, 2].imshow(edges_sobel, cmap='gray')
axes[1, 2].set_title('Sobel Edge Detection')
axes[1, 3].imshow(binary_image, cmap='gray')
axes[1, 3].set_title('Binary Image')
axes[2, 0].imshow(contour_image)
axes[2, 0].set_title('Contours')
axes[2, 1].imshow(image_with_faces)
axes[2, 1].set_title('Face Detection')
axes[2, 2].imshow(image_cv)
axes[2, 2].set_title('Original Image')
plt.tight_layout()
plt.show()
