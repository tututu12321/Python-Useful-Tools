import nltk
import spacy
import gensim
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from transformers import pipeline
import re
import nltk

# punkt_tabのダウンロード
nltk.download('punkt_tab')

# NLTKリソースのダウンロード
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# ストップワードとトークン化
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

# サンプルテキスト
text = "Hello! How are you today? I'm learning NLP in Python. Python is great for NLP tasks!"

# トークン化
tokens = word_tokenize(text)
print("Tokens:", tokens)

# ストップワードの削除
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print("Filtered Tokens:", filtered_tokens)

# ステミング
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_tokens]
print("Stemmed Words:", stemmed_words)

# レンマタイゼーション
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("Lemmatized Words:", lemmatized_words)

# 正規化（小文字化、句読点除去）
normalized_text = text.lower()
normalized_text = re.sub(r'[^\w\s]', '', normalized_text)
print("Normalized Text:", normalized_text)

# spaCyの準備
nlp = spacy.load("en_core_web_sm")
doc = nlp(text)

# spaCyによる固有表現抽出
print("Named Entities:", [(ent.text, ent.label_) for ent in doc.ents])

# Word2Vecによる単語のベクトル化
sentences = [['hello', 'how', 'are', 'you'], ['python', 'is', 'great', 'for', 'nlp']]
model = Word2Vec(sentences, min_count=1)
word_vector = model.wv['python']
print("Word Vector for 'python':", word_vector)

# TF-IDFによる単語の重要度
documents = [
    "I love programming in Python.",
    "Python programming is fun and exciting.",
    "I enjoy solving problems using Python."
]
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)
print("TF-IDF Matrix:")
print(tfidf_matrix.toarray())

# Bag of Words (BoW)による単語のベクトル化
vectorizer_bow = CountVectorizer()
bow_matrix = vectorizer_bow.fit_transform(documents)
print("BoW Matrix:")
print(bow_matrix.toarray())

# Hugging Faceのtransformersライブラリによる感情分析
classifier = pipeline('sentiment-analysis')
result = classifier("I love programming in Python!")
print("Sentiment Analysis Result:", result)

