import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error, accuracy_score

# サンプルデータの生成
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
df = pd.read_csv(url, header=None, names=columns)

# 1. 欠損データの処理
df.loc[0, 'sepal_length'] = np.nan  # 欠損データを挿入
df['sepal_length'].fillna(df['sepal_length'].mean(), inplace=True)  # 平均で欠損値を埋める

# 2. 異常値の処理（zスコアによるフィルタリング）
z_scores = np.abs((df['sepal_length'] - df['sepal_length'].mean()) / df['sepal_length'].std())
df = df[z_scores < 3]  # zスコアが3未満のデータのみ残す

# 3. 時系列データの作成（ここではダミー時系列データを生成）
df['date'] = pd.date_range(start='2021-01-01', periods=len(df), freq='D')
df.set_index('date', inplace=True)

# 4. データ可視化（Matplotlib & Seaborn）
plt.figure(figsize=(10,6))
sns.boxplot(x='class', y='sepal_length', data=df)
plt.title('Boxplot of Sepal Length by Class')
plt.show()

plt.figure(figsize=(10,6))
sns.pairplot(df, hue='class')
plt.show()

# 5. 機械学習アルゴリズム
X = df.drop(columns=['class'])
y = df['sepal_length']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 重回帰分析
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
print(f"Linear Regression MSE: {mean_squared_error(y_test, y_pred)}")

# ロジスティック回帰（分類）
y_class = df['class'].astype('category').cat.codes
X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42)

logreg = LogisticRegression(max_iter=200)
logreg.fit(X_train, y_train)
y_pred_class = logreg.predict(X_test)
print(f"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_class)}")

# 決定木（分類）
tree = DecisionTreeClassifier()
tree.fit(X_train, y_train)
y_pred_tree = tree.predict(X_test)
print(f"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_tree)}")

# k-NN（k近傍法）
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
print(f"k-NN Accuracy: {accuracy_score(y_test, y_pred_knn)}")

# クラスタリング（k-means）
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3)
df['cluster'] = kmeans.fit_predict(X)
plt.scatter(df['sepal_length'], df['sepal_width'], c=df['cluster'], cmap='viridis')
plt.title('K-Means Clustering')
plt.show()

# 主成分分析（PCA）
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_components = pca.fit_transform(X)
plt.scatter(pca_components[:, 0], pca_components[:, 1], c=df['class'].astype('category').cat.codes, cmap='viridis')
plt.title('PCA of Iris Dataset')
plt.show()

